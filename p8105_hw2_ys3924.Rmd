---
title: "p8105_hw2_ys3924"
author: "YISU"
date: "2024-10-02"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
library(readxl)
```

# Problem 1

## Read in and clean NYC Transit data

```{r, message=FALSE, warning=FALSE}
NYC_transit = 
  read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
           col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

The NYC Transit dataset contains subway line, station name, station latitude and longitude, routes served, entry, vending, entrance type, and ADA compliance.

In my data cleaning process, I first used janitor::clean_names() function to convert all column names to lowercase. Then I selected the key factors needed for this analysis. Finally, I transformed the entry variable from a "YES"/"NO" character format to a logical TRUE/FALSE. I also changed the column types for routes when importing the dataset.

There are 1868 rows and 19 columns for this resulting dataset. 

## Count distinct stations

```{r}
num_distinct_stations = NYC_transit |>
  distinct(station_name, line) |>
  nrow()
```

There are `r num_distinct_stations` distinct stations.

## Count ADA stations 

```{r}
num_ada = NYC_transit |>
  filter(ada == TRUE) |>
  distinct(station_name, line) |>
  nrow()
```

There are `r num_ada` ADA compliant stations.

## Calculate proportion of station entrances / exits without vending allow entrance

```{r}
num_no_vending = NYC_transit |>
  filter(vending == "NO") |>
  pull(entry) |> 
  mean()
```

The proportion of station entrances / exits without vending allow entrance is `r num_no_vending`.

```{r}
num_A_train = NYC_transit |> 
  pivot_longer(
    cols = starts_with("route"),
    names_to = "route_name",
    values_to = "route"
  ) |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct() |>
  nrow()
```

I used pivot_longer() function to change the format. There are `r num_A_train` distinct stations serve the A train.

```{r}
num_A_train_ADA = NYC_transit |> 
  pivot_longer(
    cols = starts_with("route"),
    names_to = "route_name",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct() |>
  nrow()
```

Of the stations that serve the A train, `r num_A_train_ADA` of them are ADA compliant.


# Problem 2

## Read in and clean Mr.Trash Wheel data

```{r, message=FALSE}
mr_trash_wheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                            sheet = "Mr. Trash Wheel",
                            na = c("NA", ".", ""), 
                            skip = 1) |>
  janitor::clean_names() |>
  select(dumpster:homes_powered) |>
  mutate(sports_balls = as.integer(round(sports_balls, 0)),
         trash_wheel_name = "Mr. Trash Wheel",
         year = as.numeric(year)) |>
  filter(!is.na(dumpster)) |>
  relocate(trash_wheel_name, dumpster)
```

## Read in and clean Professor Trash Wheel data

```{r}
professor_trash_wheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                                   sheet = "Professor Trash Wheel", 
                                   na = c("NA", ".", ""), 
                                   skip = 1) |>
  janitor::clean_names() |>
  mutate(trash_wheel_name = "Professor Trash Wheel") |>
  filter(!is.na(dumpster)) |>
  slice(1:(n() - 1)) |>
  relocate(trash_wheel_name, dumpster)
```

## Read in and clean Gwynnda Trash Wheel data

```{r}
gwynnda_trash_wheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                                 sheet = "Gwynnda Trash Wheel", 
                                 na = c("NA", ".", ""), 
                                 skip = 1) |>
  janitor::clean_names() |>
  mutate(trash_wheel_name = "Gwynnda Trash Wheel") |>
  filter(!is.na(dumpster)) |>
  relocate(trash_wheel_name, dumpster)
```

## Combine the dataset

```{r}
full_trash_wheel = bind_rows(mr_trash_wheel, professor_trash_wheel, gwynnda_trash_wheel)
num_rows = nrow(full_trash_wheel)
num_cols = ncol(full_trash_wheel)
```

The dataset contains information on trash collection by Mr.Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel. There are `r num_rows` rows and `r num_cols` columns in this dataset. The key variables include year, weight_tons, volumne_cubic_yard, plastic bottles, cigarette_butts, and homes_powered. 

```{r}
prof_total_weight = professor_trash_wheel |>
  summarise(prof_total_weight = sum(weight_tons, na.rm = TRUE)) |>
  pull(prof_total_weight)
gwynnda_cigarette_butts <- gwynnda_trash_wheel |>
  filter(month == "June" & year == 2022) |>
  summarise(gwynnda_cigarette_butts = sum(cigarette_butts, na.rm = TRUE)) |>
  pull(gwynnda_cigarette_butts)
```

The total weight of trash collected by Professor Trash Wheel was `r prof_total_weight`. The total number of cigarette butts collected by Gwynnda in June of 2022 was `r gwynnda_cigarette_butts`.

# Problem 3

## Read in and clean the datasets

```{r, message=FALSE}
bakers = read_csv("./data/gbb_datasets/bakers.csv") |>
  janitor::clean_names()   |>          
  rename(baker = baker_name) |>
  separate(baker, into = c("baker", "last_name"), sep = " ", extra = "drop") 
bakes = read_csv("./data/gbb_datasets/bakes.csv") |>
  janitor::clean_names()
results = read_csv("./data/gbb_datasets/results.csv", skip = 1) |>
  janitor::clean_names() |>
  janitor::row_to_names(row_number = 1) |>
  mutate(series = as.numeric(series),
         episode = as.numeric(episode))
```
In my data cleaning process, I first used janitor::clean_names() function to convert all column names to lowercase. For the bakers dataset, I renamed the baker_names column to baker and split the full name, retaining only the first name to ensure consistency with the other datasets. For the results dataset, I skipped the first row during the import process since it contains NA values in all columns. I then set the actual row with column names as the proper column headers. Finally, I changed the series and episode column in results dataset to numeric to keep consistency.



## Check for completeness and correctness across datasets 
```{r}
anti_join(bakers, bakes)
anti_join(bakes, bakers)
anti_join(bakers, results)
```

By looking at above anti_join() results, the baker "Jo" from series 2 is not matched in results dataset. In bakes dataset, their name appears with quotes, which should be fixed. Also, bakes dataset lacks data from series 9 and 10, so bakers and results from those series are missing.

```{r}
bakes <- bakes |>
  mutate(baker = ifelse(baker == "\"Jo\"", "Jo", baker))
```
In order to solve the problem, I replaced "Jo" with Jo in the bakes dataset.


## Merge the datasets

```{r}
gbb_combined = left_join(bakers, results, by = c("baker", "series")) |>
  left_join(bakes, by = c("baker", "series", "episode")) |>
  relocate(baker, last_name, series, episode) |>
  arrange(series)
write_csv(gbb_combined, "./data/gbb_datasets/gbb_combined.csv")
num_gbb_row = nrow(gbb_combined)
num_gbb_col = ncol(gbb_combined)
```


In the combined dataset, I arranged the columns in the order of first name, last name, series, and episode. I also sorted the series to improve readability. The final dataset includes `r num_gbb_row` observations and `r num_gbb_col` variables, providing details on bakers' age, occupation, hometown, as well as their bakes and results.


## Create table for Star Baker or Winner

```{r}
star_bakers <- gbb_combined |>
  filter(series >= 5 & series <= 10, result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, result) |>
  knitr::kable()
star_bakers
```

By looking at the table, the predicted winner consistently earned the title of Star Baker at least once in every season except for Season 10. In season 5, Richard won Star Baker multiple times, but he was not the winner. While multiple Star Baker titles can indicate strong performance, they do not always predict the final winner.

## Read in and clean viewership data

```{r, message=FALSE}
viewers = read_csv("./data/gbb_datasets/viewers.csv") |>
  janitor::clean_names() |>
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewer"
  ) |>
  relocate(series, episode, viewer) |>
  arrange(series)
head(viewers, 10)
# Calculate the average viewership for Season 1
avg_viewer_1 <- viewers |>
  filter(series == "series_1") |>
  summarise(avg_viewer_1 = mean(viewer, na.rm = TRUE)) |>
  pull(avg_viewer_1)
# Calculate the average viewership for Season 5
avg_viewer_5 <- viewers |>
  filter(series == "series_5") |>
  summarise(avg_viewer_5 = mean(viewer, na.rm = TRUE)) |>
  pull(avg_viewer_5)
```
I used janitor::clean_names() function to convert all column names to lowercase. I then used the pivot_longer() function to reformat the data to improve its readability. I rearranged the columns in the order of series, episode, and viewer. Finally, I sorted the data by the series column.

The average viewership in Season 1 is `r avg_viewer_1`.
The average viewership in Season 5 is `r avg_viewer_5`.