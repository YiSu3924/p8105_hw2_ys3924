p8105_hw2_ys3924
================
YISU
2024-10-02

``` r
library(tidyverse)
library(readxl)
```

# Problem 1

## Read in and clean NYC Transit data

``` r
NYC_transit = 
  read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |>
  janitor::clean_names() |>
  select(line:entry, vending, ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

The NYC Transit dataset contains subway line, station name, station
latitude and longitude, routes served, entry, vending, entrance type,
and ADA compliance.

In my data cleaning process, I first used janitor::clean_names()
function to convert all column names to lowercase. Then I selected the
key factors needed for this analysis. Finally, I transformed the entry
variable from a “YES”/“NO” character format to a logical TRUE/FALSE.

There are 1868 rows and 19 columns for this resulting dataset.

## Count distinct stations

``` r
num_distinct_stations = NYC_transit |>
  distinct(station_name, line) |>
  nrow()
```

There are 465 distinct stations.

## Count ADA stations

``` r
num_ada = NYC_transit |>
  filter(ada == TRUE) |>
  distinct(station_name, line) |>
  nrow()
```

There are 84 ADA compliant stations.

## Calculate proportion of station entrances / exits without vending allow entrance

``` r
num_no_vending = NYC_transit |>
  filter(vending == "NO") |>
  nrow()
proportion_no_vending = num_no_vending / nrow(NYC_transit)
```

The proportion of station entrances / exits without vending allow
entrance is 0.0979657.

# Problem 2

## Read in and clean Mr.Trash Wheel data

``` r
mr_trash_wheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", na = c("NA", ".", ""), skip = 1) |>
  janitor::clean_names() |>
  select(dumpster:homes_powered) |>
  mutate(sports_balls = as.integer(round(sports_balls, 0)),
         trash_wheel_name = "Mr. Trash Wheel",
         year = as.numeric(year)) |>
  filter(!is.na(dumpster)) |>
  relocate(trash_wheel_name, dumpster)
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

## Read in and clean Professor Trash Wheel data

``` r
professor_trash_wheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", na = c("NA", ".", ""), skip = 1) |>
  janitor::clean_names() |>
  mutate(trash_wheel_name = "Professor Trash Wheel") |>
  filter(!is.na(dumpster)) |>
  slice(1:(n() - 1)) |>
  relocate(trash_wheel_name, dumpster)
```

## Read in and clean Gwynnda Trash Wheel data

``` r
gwynnda_trash_wheel = read_excel("data/202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", na = c("NA", ".", ""), skip = 1) |>
  janitor::clean_names() |>
  mutate(trash_wheel_name = "Gwynnda Trash Wheel") |>
  filter(!is.na(dumpster)) |>
  relocate(trash_wheel_name, dumpster)
```

## Combine the dataset

``` r
full_trash_wheel = bind_rows(mr_trash_wheel, professor_trash_wheel, gwynnda_trash_wheel)
num_rows = nrow(full_trash_wheel)
num_cols = ncol(full_trash_wheel)
```

The dataset contains information on trash collection by Mr.Trash Wheel,
Professor Trash Wheel, and Gwynnda Trash Wheel. There are 1032 rows and
15 columns in this dataset. The key variables include year, weight_tons,
volumne_cubic_yard, plastic bottles, cigarette_butts, and homes_powered.

``` r
prof_total_weight = professor_trash_wheel |>
  summarise(prof_total_weight = sum(weight_tons, na.rm = TRUE)) |>
  pull(prof_total_weight)
gwynnda_cigarette_butts <- gwynnda_trash_wheel |>
  filter(month == "June" & year == 2022) |>
  summarise(gwynnda_cigarette_butts = sum(cigarette_butts, na.rm = TRUE)) |>
  pull(gwynnda_cigarette_butts)
```

The total weight of trash collected by Professor Trash Wheel was 246.74.
The total number of cigarette butts collected by Gwynnda in June of 2022
was 1.812^{4}.

# Problem 3

## Read in and clean the datasets

``` r
bakers = read_csv("./data/gbb_datasets/bakers.csv") |>
  janitor::clean_names()   |>          
  rename(baker = baker_name) |>
  separate(baker, into = c("baker", "last_name"), sep = " ", extra = "drop") 
bakes = read_csv("./data/gbb_datasets/bakes.csv") |>
  janitor::clean_names()
results = read_csv("./data/gbb_datasets/results.csv", skip = 1) |>
  janitor::clean_names() |>
  janitor::row_to_names(row_number = 1) |>
  mutate(series = as.numeric(series),
         episode = as.numeric(episode))
```

In my data cleaning process, I first used janitor::clean_names()
function to convert all column names to lowercase. For the bakers
dataset, I renamed the baker_names column to baker and split the full
name, retaining only the first name to ensure consistency with the other
datasets. For the results dataset, I skipped the first row during the
import process since it contains NA values in all columns. I then set
the actual row with column names as the proper column headers. Finally,
I changed the series and episode column in results dataset to numeric to
keep consistency.

## Check for completeness and correctness across datasets

``` r
anti_join(bakers, bakes)
```

    ## Joining with `by = join_by(baker, series)`

    ## # A tibble: 26 × 6
    ##    baker  last_name       series baker_age baker_occupation             hometown
    ##    <chr>  <chr>            <dbl>     <dbl> <chr>                        <chr>   
    ##  1 Alice  Fevronia            10        28 Geography teacher            Essex   
    ##  2 Amelia LeBruin             10        24 Fashion designer             Halifax 
    ##  3 Antony Amourdoux            9        30 Banker                       London  
    ##  4 Briony Williams             9        33 Full-time parent             Bristol 
    ##  5 Dan    Beasley-Harling      9        36 Full-time parent             London  
    ##  6 Dan    Chambers            10        32 Support worker               Rotherh…
    ##  7 David  Atherton            10        36 International health adviser Whitby  
    ##  8 Helena Garcia              10        40 Online project manager       Leeds   
    ##  9 Henry  Bird                10        20 Student                      Durham  
    ## 10 Imelda McCarron             9        33 Countryside recreation offi… County …
    ## # ℹ 16 more rows

``` r
anti_join(bakes, bakers)
```

    ## Joining with `by = join_by(series, baker)`

    ## # A tibble: 8 × 5
    ##   series episode baker    signature_bake                            show_stopper
    ##    <dbl>   <dbl> <chr>    <chr>                                     <chr>       
    ## 1      2       1 "\"Jo\"" Chocolate Orange CupcakesOrange and Card… Chocolate a…
    ## 2      2       2 "\"Jo\"" Caramelised Onion, Gruyere and Thyme Qui… Raspberry a…
    ## 3      2       3 "\"Jo\"" Stromboli flavored with Mozzarella, Ham,… Unknown     
    ## 4      2       4 "\"Jo\"" Lavender Biscuits                         Blueberry M…
    ## 5      2       5 "\"Jo\"" Salmon and Asparagus Pie                  Apple and R…
    ## 6      2       6 "\"Jo\"" Rum and Raisin Baked Cheesecake           Limoncello …
    ## 7      2       7 "\"Jo\"" Raspberry & Strawberry Mousse Cake        Pain Aux Ra…
    ## 8      2       8 "\"Jo\"" Raspberry and Blueberry Mille Feuille     Mini Victor…

``` r
anti_join(bakers, results)
```

    ## Joining with `by = join_by(baker, series)`

    ## # A tibble: 1 × 6
    ##   baker last_name series baker_age baker_occupation hometown    
    ##   <chr> <chr>      <dbl>     <dbl> <chr>            <chr>       
    ## 1 Jo    Wheatley       2        41 Housewife        Ongar, Essex

By looking at above anti_join() results, the baker “Jo” from series 2 is
not matched in results dataset. In bakes dataset, their name appears
with quotes, which should be fixed. Also, bakes dataset lacks data from
series 9 and 10, so bakers and results from those series are missing.

``` r
bakes <- bakes |>
  mutate(baker = ifelse(baker == "\"Jo\"", "Jo", baker))
```

In order to solve the problem, I replaced “Jo” with Jo in the bakes
dataset.

## Merge the datasets

``` r
gbb_combined = left_join(bakers, results) |>
  left_join(bakes) |>
  relocate(baker, last_name, series, episode) |>
  arrange(series)
```

    ## Joining with `by = join_by(baker, series)`
    ## Joining with `by = join_by(baker, series, episode)`

``` r
write_csv(gbb_combined, "./data/gbb_datasets/gbb_combined.csv")
```

In the combined dataset, I arranged the columns in the order of first
name, last name, series, and episode. I also sorted the series to
improve readability. The final dataset includes 1129 observations and 11
variables, providing details on bakers’ age, occupation, hometown, as
well as their bakes and results.

## Create table for Star Baker or Winner

``` r
star_bakers <- gbb_combined |>
  filter(series >= 5 & series <= 10, result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, result)
star_bakers
```

    ## # A tibble: 60 × 4
    ##    series episode baker   result    
    ##     <dbl>   <dbl> <chr>   <chr>     
    ##  1      5       6 Chetna  STAR BAKER
    ##  2      5       5 Kate    STAR BAKER
    ##  3      5       3 Luis    STAR BAKER
    ##  4      5       1 Nancy   STAR BAKER
    ##  5      5      10 Nancy   WINNER    
    ##  6      5       2 Richard STAR BAKER
    ##  7      5       4 Richard STAR BAKER
    ##  8      5       7 Richard STAR BAKER
    ##  9      5       8 Richard STAR BAKER
    ## 10      5       9 Richard STAR BAKER
    ## # ℹ 50 more rows

By looking at the table, the predicted winner consistently earned the
title of Star Baker at least once in every season except for Season 10.
In season 5, Richard won Star Baker multiple times, but he was not the
winner. While multiple Star Baker titles can indicate strong
performance, they do not always predict the final winner.
